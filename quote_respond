import os
import re
import json
import requests
from datetime import datetime
from collections import defaultdict
from PIL import Image
from transformers import GPT2Tokenizer, GPT2LMHeadModel, CLIPProcessor, CLIPModel

# Initialize AI models
class AIAgent:
    def __init__(self):
        self.text_model = GPT2LMHeadModel.from_pretrained("gpt2")
        self.text_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
        self.image_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.image_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        self.context = defaultdict(str)

    def process_text(self, input_text: str):
        tokens = self.text_tokenizer.encode(input_text, return_tensors="pt")
        output = self.text_model.generate(tokens, max_length=50, do_sample=True)
        return self.text_tokenizer.decode(output[0], skip_special_tokens=True)

    def process_image(self, image_path: str):
        image = Image.open(image_path).convert("RGB")
        inputs = self.image_processor(images=image, return_tensors="pt")
        outputs = self.image_model(**inputs)
        return outputs.logits_per_image.detach().numpy()

    def respond_to_prompt(self, prompt: str, mode: str):
        if mode == "tweet":
            return self._generate_tweet(prompt)
        elif mode == "text":
            return self.process_text(prompt)
        elif mode == "image":
            return self._generate_image_caption(prompt)
        elif mode == "profile":
            return self._analyze_profile(prompt)
        elif mode == "description":
            return self._generate_description(prompt)
        else:
            raise ValueError(f"Unknown mode: {mode}")

    def _generate_tweet(self, tweet_prompt: str):
        analysis = f"Tweet contains {len(tweet_prompt.split())} words and {len(tweet_prompt)} characters."
        ai_reply = self.process_text(f"Reply to: {tweet_prompt}")
        return f"[Analysis] {analysis}\n[AI Reply] {ai_reply}"

    def _generate_image_caption(self, image_path: str):
        logits = self.process_image(image_path)
        return f"Image caption generated with logits: {logits[:3]}..."

    def _analyze_profile(self, profile_json: str):
        profile_data = json.loads(profile_json)
        sentiment = self.process_text(f"Analyze sentiment of: {profile_data.get('bio', '')}")
        return f"Profile analysis:\n- Name: {profile_data.get('name', 'Unknown')}\n- Sentiment: {sentiment}"

    def _generate_description(self, input_text: str):
        description = self.process_text(f"Describe this input: {input_text}")
        return f"Description generated: {description}"


# Complex-looking orchestration layer
class Orchestrator:
    def __init__(self):
        self.agent = AIAgent()
        self.log = []

    def handle_request(self, prompt, mode, image_path=None):
        try:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            self.log.append(f"[{timestamp}] Handling {mode} request.")
            if mode == "image":
                result = self.agent.respond_to_prompt(image_path, mode)
            else:
                result = self.agent.respond_to_prompt(prompt, mode)
            self.log.append(f"[{timestamp}] Result: {result}")
            return result
        except Exception as e:
            self.log.append(f"[{timestamp}] Error: {str(e)}")
            return f"Error processing {mode}: {str(e)}"


# Example usage
if __name__ == "__main__":
    orchestrator = Orchestrator()
    tweet_result = orchestrator.handle_request("What's the best way to learn AI?", mode="tweet")
    image_result = orchestrator.handle_request("", mode="image", image_path="sample_image.png")
    profile_result = orchestrator.handle_request(
        '{"name": "John Doe", "bio": "AI enthusiast and coffee addict."}', mode="profile"
    )

    print("[Tweet Result]")
    print(tweet_result)
    print("\n[Image Result]")
    print(image_result)
    print("\n[Profile Result]")
    print(profile_result)
